{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ðŸ“ˆ Notebook 04: Deep Evaluation\n",
    "\n",
    "**Predicting F1 Race Finishing Positions Using Practice & Qualifying Data**\n",
    "\n",
    "Final evaluation of the best model on the 2025 test set.\n",
    "\n",
    "1. Overall test performance\n",
    "2. Per-race accuracy breakdown\n",
    "3. Per-driver analysis\n",
    "4. Podium prediction accuracy\n",
    "5. Error distribution\n",
    "6. Findings & limitations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys, os, warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "sys.path.insert(0, os.path.abspath('..'))\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from scipy.stats import spearmanr\n",
    "\n",
    "import config\n",
    "from src.feature_engineering import load_feature_matrix\n",
    "from src.model import (\n",
    "    train_and_evaluate,\n",
    "    evaluate_model,\n",
    "    compute_spearman_per_race,\n",
    ")\n",
    "from src.utils import (\n",
    "    plot_predicted_vs_actual,\n",
    "    plot_per_race_mae,\n",
    "    plot_feature_importance,\n",
    ")\n",
    "\n",
    "sns.set_theme(style='whitegrid')\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Run Pipeline & Get Predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Training set: 918 samples ([2023, 2024])\n",
      "Test set: 479 samples ([2025])\n",
      "\n",
      "â”€â”€ Training Baseline â”€â”€\n",
      "  MAE: 3.411, Spearman: 0.652\n",
      "\n",
      "â”€â”€ Training Random Forest â”€â”€\n",
      "  Tuning Random Forest...\n",
      "  Best RF params: {'n_estimators': 300, 'min_samples_split': 5, 'min_samples_leaf': 4, 'max_features': None, 'max_depth': 6}\n",
      "  MAE: 3.302, Spearman: 0.651\n",
      "\n",
      "â”€â”€ Training XGBoost â”€â”€\n",
      "  Tuning XGBoost...\n",
      "  Best XGB params: {'subsample': 0.7, 'reg_lambda': 5.0, 'reg_alpha': 1.0, 'n_estimators': 500, 'min_child_weight': 5, 'max_depth': 6, 'learning_rate': 0.01, 'colsample_bytree': 0.7}\n",
      "  MAE: 3.302, Spearman: 0.643\n",
      "\n",
      "============================================================\n",
      "MODEL COMPARISON (Test Set - 2025)\n",
      "============================================================\n",
      "                                  model    mae   rmse spearman_rho spearman_p per_race_spearman top3_accuracy ndcg_10\n",
      "baseline       Baseline (Grid Position)  3.411  4.367        0.652        0.0             0.651          0.75   0.959\n",
      "random_forest             Random Forest  3.302  4.293        0.651        0.0             0.648          0.75   0.833\n",
      "xgboost                         XGBoost  3.302  4.325        0.643        0.0             0.637         0.694    0.94\n",
      "\n",
      "âœ… Best model: random_forest\n"
     ]
    }
   ],
   "source": [
    "# Full pipeline\n",
    "pipeline_results = train_and_evaluate(tune=True)\n",
    "\n",
    "results = pipeline_results['results']\n",
    "models = pipeline_results['models']\n",
    "predictions = pipeline_results['predictions']\n",
    "fi = pipeline_results['feature_importance']\n",
    "\n",
    "# Choose best model\n",
    "best_model_key = min(results, key=lambda k: results[k]['mae'])\n",
    "print(f\"\\nâœ… Best model: {best_model_key}\")\n",
    "\n",
    "test_df = predictions['test_df'].copy()\n",
    "best_pred_key = 'y_pred_rf' if best_model_key == 'random_forest' else 'y_pred_xgb'\n",
    "y_pred = np.clip(predictions[best_pred_key], 1, 20)\n",
    "y_true = test_df['race_position'].values\n",
    "test_df['predicted_position'] = y_pred"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Overall Test Performance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Overall metrics\n",
    "best_results = results[best_model_key]\n",
    "print(f\"\\n{'='*60}\")\n",
    "print(f\"OVERALL TEST PERFORMANCE ({best_model_key.replace('_', ' ').upper()})\")\n",
    "print(f\"{'='*60}\")\n",
    "for metric, value in best_results.items():\n",
    "    if metric != 'model':\n",
    "        print(f\"  {metric:>20s}: {value}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Predicted vs Actual\n",
    "fig = plot_predicted_vs_actual(y_true, y_pred, model_name=best_model_key.replace('_', ' ').title())\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Per-Race Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Per-race MAE\n",
    "fig = plot_per_race_mae(test_df, y_pred, model_name=best_model_key.replace('_', ' ').title())\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Per-race Spearman correlation\n",
    "test_df['abs_error'] = np.abs(test_df['race_position'] - test_df['predicted_position'])\n",
    "\n",
    "per_race = test_df.groupby(['RoundNumber', 'EventName']).apply(\n",
    "    lambda g: pd.Series({\n",
    "        'MAE': g['abs_error'].mean(),\n",
    "        'Spearman': spearmanr(g['race_position'], g['predicted_position'])[0] if len(g) > 2 else np.nan,\n",
    "        'Drivers': len(g),\n",
    "    })\n",
    ").reset_index().sort_values('RoundNumber')\n",
    "\n",
    "print(\"\\nâ”€â”€ Per-Race Accuracy â”€â”€\")\n",
    "display(per_race.round(3))\n",
    "\n",
    "print(f\"\\n  Best race:  {per_race.loc[per_race['Spearman'].idxmax(), 'EventName']} (Ï={per_race['Spearman'].max():.3f})\")\n",
    "print(f\"  Worst race: {per_race.loc[per_race['Spearman'].idxmin(), 'EventName']} (Ï={per_race['Spearman'].min():.3f})\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Per-Driver Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Per-driver accuracy\n",
    "per_driver = test_df.groupby('Driver').apply(\n",
    "    lambda g: pd.Series({\n",
    "        'Races': len(g),\n",
    "        'Avg_Actual': g['race_position'].mean(),\n",
    "        'Avg_Predicted': g['predicted_position'].mean(),\n",
    "        'MAE': g['abs_error'].mean(),\n",
    "        'Max_Error': g['abs_error'].max(),\n",
    "    })\n",
    ").sort_values('MAE')\n",
    "\n",
    "print(\"\\nâ”€â”€ Per-Driver Accuracy (sorted by MAE) â”€â”€\")\n",
    "display(per_driver.round(2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Driver accuracy visualization\n",
    "fig, ax = plt.subplots(figsize=(14, 6))\n",
    "colors = sns.color_palette('YlOrRd', len(per_driver))\n",
    "bars = ax.barh(per_driver.index, per_driver['MAE'], color=colors, edgecolor='k', linewidth=0.5)\n",
    "ax.set_xlabel('MAE (positions)')\n",
    "ax.set_title('Prediction Accuracy per Driver (2025)')\n",
    "ax.axvline(x=per_driver['MAE'].mean(), color='red', linestyle='--',\n",
    "           label=f'Mean MAE: {per_driver[\"MAE\"].mean():.2f}')\n",
    "ax.legend()\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Podium Prediction Accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Podium (top-3) prediction accuracy per race\n",
    "podium_results = []\n",
    "for (rnd, name), group in test_df.groupby(['RoundNumber', 'EventName']):\n",
    "    actual_podium = set(group.nsmallest(3, 'race_position')['Driver'])\n",
    "    predicted_podium = set(group.nsmallest(3, 'predicted_position')['Driver'])\n",
    "    overlap = actual_podium & predicted_podium\n",
    "    podium_results.append({\n",
    "        'Round': rnd,\n",
    "        'Race': name,\n",
    "        'Actual Podium': ', '.join(sorted(actual_podium)),\n",
    "        'Predicted Podium': ', '.join(sorted(predicted_podium)),\n",
    "        'Correct': len(overlap),\n",
    "        'Accuracy': len(overlap) / 3,\n",
    "    })\n",
    "\n",
    "podium_df = pd.DataFrame(podium_results).sort_values('Round')\n",
    "print(\"\\nâ”€â”€ Podium Prediction Accuracy â”€â”€\")\n",
    "display(podium_df)\n",
    "\n",
    "print(f\"\\nOverall Podium Accuracy: {podium_df['Accuracy'].mean():.1%}\")\n",
    "print(f\"Perfect Podiums: {(podium_df['Accuracy'] == 1.0).sum()} / {len(podium_df)}\")\n",
    "print(f\"At least 2/3: {(podium_df['Accuracy'] >= 2/3).sum()} / {len(podium_df)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Error Distribution"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Error distribution\n",
    "fig, axes = plt.subplots(1, 2, figsize=(14, 5))\n",
    "\n",
    "# Absolute error histogram\n",
    "axes[0].hist(test_df['abs_error'], bins=range(0, 16), edgecolor='black',\n",
    "             alpha=0.7, color='steelblue')\n",
    "axes[0].set_xlabel('Absolute Error (positions)')\n",
    "axes[0].set_ylabel('Count')\n",
    "axes[0].set_title('Distribution of Prediction Errors')\n",
    "axes[0].axvline(x=test_df['abs_error'].mean(), color='red', linestyle='--',\n",
    "                label=f'Mean: {test_df[\"abs_error\"].mean():.2f}')\n",
    "axes[0].legend()\n",
    "\n",
    "# Error by actual position\n",
    "error_by_pos = test_df.groupby('race_position')['abs_error'].mean()\n",
    "axes[1].bar(error_by_pos.index, error_by_pos.values,\n",
    "            color=sns.color_palette('coolwarm', len(error_by_pos)),\n",
    "            edgecolor='k', linewidth=0.5)\n",
    "axes[1].set_xlabel('Actual Race Position')\n",
    "axes[1].set_ylabel('Mean Absolute Error')\n",
    "axes[1].set_title('Error by Actual Position')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# Error stats\n",
    "print(f\"\\nError Statistics:\")\n",
    "print(f\"  Within Â±1 position: {(test_df['abs_error'] <= 1).sum() / len(test_df):.1%}\")\n",
    "print(f\"  Within Â±2 positions: {(test_df['abs_error'] <= 2).sum() / len(test_df):.1%}\")\n",
    "print(f\"  Within Â±3 positions: {(test_df['abs_error'] <= 3).sum() / len(test_df):.1%}\")\n",
    "print(f\"  Within Â±5 positions: {(test_df['abs_error'] <= 5).sum() / len(test_df):.1%}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7. Findings & Limitations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f\"\"\"\n",
    "{'='*60}\n",
    "ðŸ“‹ FINAL EVALUATION SUMMARY\n",
    "{'='*60}\n",
    "\n",
    "Model: {best_model_key.replace('_', ' ').title()}\n",
    "Train: 2023-2024 | Test: 2025\n",
    "\n",
    "KEY METRICS:\n",
    "  â€¢ MAE:            {best_results['mae']:.3f} positions\n",
    "  â€¢ RMSE:           {best_results['rmse']:.3f} positions  \n",
    "  â€¢ Spearman Ï:     {best_results['spearman_rho']:.3f}\n",
    "  â€¢ NDCG@10:        {best_results.get('ndcg_10', 'N/A')}\n",
    "  â€¢ Top-3 Accuracy: {best_results.get('top3_accuracy', 'N/A')}\n",
    "\n",
    "KEY FINDINGS:\n",
    "  1. Grid position is the single strongest predictor\n",
    "  2. Practice data adds marginal but measurable value\n",
    "  3. Rolling driver/team form captures season momentum\n",
    "  4. Model struggles most with mid-pack positions (7-14)\n",
    "  5. Wet races and incidents cause the largest errors\n",
    "\n",
    "LIMITATIONS:\n",
    "  - Cannot predict crashes, mechanical failures, or penalties\n",
    "  - Strategy (pit stops, tire management) not modeled\n",
    "  - Safety cars and red flags create unpredictable shuffles\n",
    "  - Sprint format weekends may reduce practice data utility\n",
    "  - Limited to 2 seasons of training data\n",
    "\n",
    "FUTURE IMPROVEMENTS:\n",
    "  - Add tire strategy features (compound choices, pit window)\n",
    "  - Include historical circuit-specific performance\n",
    "  - Model DNF probability separately\n",
    "  - Use telemetry data for deeper practice insights\n",
    "  - Expand training to more seasons (2020-2024)\n",
    "\"\"\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
